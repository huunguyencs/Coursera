{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification and Representation\n",
    "\n",
    "- **Classification**\n",
    "\n",
    "    - The classification is just look like regression problem, except that the values we now want to predict take on only s small number of discrete values.\n",
    "\n",
    "    - Focus on the **binary classification problem** in which y can take on only 2 values 0 and 1. 0 is called \"negative class\" (-), and 1 the \"positive class\" (+).\n",
    "\n",
    "- **Hypothesis Representation**\n",
    "\n",
    "    - Let's change the form for our hypothesis $h_\\theta(x)$ to satisfy $0 \\le h_\\theta(x) \\le 1$. This is accomplished by plugging $\\theta^Tx$ into the Logistic Function (or Sigmoid Function):\n",
    "\n",
    "        $$h_\\theta(x) = g(\\theta^Tx)$$\n",
    "\n",
    "        $$g(z) = \\dfrac{1}{1 + e^{-z}}$$\n",
    "\n",
    "        $$z = \\theta^Tx$$\n",
    "    \n",
    "    - With the sigmoid function, when z go to $-\\infty$, h will go to 0 (actually when z=-5, h will be very near 0) and when z go to $+\\infty$, h will go to 1 (actually when z=5, h will be very near 1).\n",
    "\n",
    "    - $h_\\theta(x)$ will give us the **probability** that our output is 1. For example, $h_\\theta(x) = 0.7$ gives us a probability of 70% that our output is 1 and 30% that our output is 0.\n",
    "\n",
    "        $h_\\theta(x) = P(y=1|x;\\theta) = 1 - P(y = 0|x;\\theta)$ \n",
    "\n",
    "- **Decision Boundary**\n",
    "\n",
    "    - In order to get our discrete 0 or 1 classification, we can translate the output of the hypothesis function as follows:\n",
    "\n",
    "        $h_\\theta(x) \\ge 0.5 \\to y = 1$\n",
    "\n",
    "        $h_\\theta(x) < 0.5 \\to y = 0$\n",
    "    \n",
    "    - And: $g(z) \\ge 0.5$ when $z \\ge 0$\n",
    "\n",
    "    - Remind:\n",
    "\n",
    "        $z = 0, e^0 = 1 \\Rightarrow g(z) = 1/2$\n",
    "\n",
    "        $z \\to \\infty, e^{-\\infty} \\Rightarrow g(z) = 1$\n",
    "\n",
    "        $z \\to -\\infty, e^\\infty \\Rightarrow g(z) = 0$\n",
    "\n",
    "    - So:\n",
    "\n",
    "        $\\theta^Tx \\ge 0 \\Rightarrow y = 1$\n",
    "\n",
    "        $\\theta^Tx < 0 \\Rightarrow y = 0$\n",
    "\n",
    "    - The **decision boundary** is the line that separates the area where y = 0 and where y = 1. It is created by our hypothesis function ($\\theta^Tx = 0$).\n",
    "\n",
    "## Logistic Regression Model\n",
    "\n",
    "- **Cost Function**\n",
    "\n",
    "    - We cannot use the same cost function that we use for linear regression because Logistic Regression will cause the output to be wavy, causing many local optima and it will not be a convex function.\n",
    "\n",
    "    - Instead, out cost function for logistic regression looks like:\n",
    "\n",
    "        $J(\\theta) = \\dfrac{1}{m}\\sum_{i=1}^mCost(h_\\theta(x^{(i)}, y^{(i)})$\n",
    "\n",
    "        $Cost(h_\\theta(x), y)  = -log(h_\\theta(x))$ if y = 1\n",
    "\n",
    "        $Cost(h_\\theta(x), y)  = -log(1 - h_\\theta(x))$ if y = 0\n",
    "\n",
    "    - And:\n",
    "\n",
    "        $Cost(h_\\theta(x), y) = 0$ if $h_\\theta(x) = y$\n",
    "\n",
    "        $Cost(h_\\theta(x), y) \\to \\infty$ if y = 0 and $h_\\theta(x) \\to 1$\n",
    "\n",
    "        $Cost(h_\\theta(x), y) \\to \\infty$ if y = 1 and $h_\\theta(x) \\to 0$\n",
    "\n",
    "- **Simplified Cost Function**\n",
    "\n",
    "    - We can compress our cost function into:\n",
    "    \n",
    "        $Cost(h_\\theta(x),y) = -ylog(h_\\theta(x)) - (1-y)log(1-h_\\theta(x))$\n",
    "    \n",
    "    - We can fully write out our entire cost as follows:\n",
    "        \n",
    "        $J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^m[y^{(i)}log(h_\\theta(x^{(i)})) + (1-y^{(i)})log(1-h_\\theta(x^{(i)}))]$\n",
    "    \n",
    "    - Vectorized implementation is:\n",
    "\n",
    "        $h = g(X\\theta)$\n",
    "\n",
    "        $J(\\theta) = \\dfrac{1}{m} \\left( -y^Tlog(h) - (1-y^T)log(1-h)\\right)$\n",
    "\n",
    "    - Gradient Descent\n",
    "\n",
    "        - Repeat\n",
    "\n",
    "        $\\theta_j := \\theta_j - \\alpha\\dfrac{\\partial}{\\partial\\theta_j}J(\\theta)$\n",
    "\n",
    "        - Work out the derivative part using calculus:\n",
    "\n",
    "        $\\theta_j := \\theta - \\dfrac{\\alpha}{m}\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$\n",
    "        - And vectorized implementation is:\n",
    "\n",
    "        $\\theta := \\theta - \\dfrac{\\alpha}{m}X^T(g(X\\theta) - y)$\n",
    "\n",
    "- **Advanced Optimization**\n",
    "\n",
    "    - \"Comjugate\", \"BFGS\", and \"L-BFGS\" are more sophisticated, faster ways to optimize $\\theta$ that can be used instead of gradient descent.\n",
    "\n",
    "    - Use the library (already tested and highly optimized) is a good solution when those optimize algorithm is very complicate.\n",
    "\n",
    "    - We first need to provide a function that evaluates the following two functions for given input value $\\theta$:\n",
    "\n",
    "        $J(\\theta)$\n",
    "        \n",
    "        $\\dfrac{\\partial}{\\partial\\theta_j}J(\\theta)$\n",
    "    \n",
    "    - The function compute them like this:\n",
    "\n",
    "    ```code\n",
    "    function [jVal, gradient] = costFunction(theta)\n",
    "        jVal = [...code to compute J(theta)...]\n",
    "        gradient = [...code to compute derivative of J(theta)...]\n",
    "    end\n",
    "    ```\n",
    "    \n",
    "    - Then we can use octave's \"fminunc()\" optimization algorithm along with the \"optimset()\" function that creates an object containing the options we want to send to \"fminunc()\":\n",
    "\n",
    "    ```code\n",
    "    options = optimset('GradObj' ,'on', 'MaxIter', 100);\n",
    "    initialTheta = zeros(2,1);\n",
    "    [optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options);\n",
    "\n",
    "    ```\n",
    "\n",
    "## Multiclass Classification\n",
    "\n",
    "- **Multiclass Classification: One-vs-all**\n",
    "\n",
    "    - We will approach the classification of data when we have more than 2 categories.\n",
    "\n",
    "    $y \\in {0, 1, ..., n}$\n",
    "\n",
    "    $h_\\theta^{(0)}(x) = P(y=0|x;\\theta)$\n",
    "\n",
    "    $h_\\theta^{(1)}(x) = P(y=1|x;\\theta)$\n",
    "\n",
    "    ...\n",
    "\n",
    "    $h_\\theta^{(n)}(x) = P(y=n|x;\\theta)$\n",
    "\n",
    "    $predict = max_i(h_\\theta^{(i)}(x))$\n",
    "\n",
    "## Solving the Problem of Overfiting\n",
    "\n",
    "- **The Problem of Overfiting**\n",
    "\n",
    "    - **Underfitting**, or high bias, is when the form of our hypothesis function\n",
    "\n",
    "- **Cost Function**\n",
    "\n",
    "- **Regularized Linear Regression**\n",
    "\n",
    "- **Regularized Logistic Regression**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
