{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Welcome"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Introduce\n",
    "\n",
    "- What is Machine Learning (ML)?\n",
    "\n",
    "    - A field of study that gives computers the ability to learn without being explicitly programming. - Arthur Sammuel.\n",
    "    \n",
    "    - A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E. - Tom Mitchell.\n",
    "\n",
    "- What is Supervised Learning (SL)?\n",
    "\n",
    "    - In SL, we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output.\n",
    "\n",
    "    - SL are categorized into **regression** and **classification**.\n",
    "\n",
    "- What is Unsupervised Learning (UL)?\n",
    "\n",
    "    - UL allows to approach problems with little or no idea what our results should look like. \n",
    "\n",
    "    - We can derive structure from data where we don't necessarily know the effect of the variables. We can derive this structure by clustering the data based on relationships among the variables in the data.\n",
    "    \n",
    "    - With UL there is no feedback based on the prediction results."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Model and Cost Function\n",
    "\n",
    "- **Model**\n",
    "\n",
    "    Training set -> Learning algorithm -> **h**\n",
    "\n",
    "    x -> **h** -> predicted y\n",
    "\n",
    "- **Cost function**\n",
    "\n",
    "    $J(\\theta_0, \\theta_1) = \\dfrac{1}{2m}\\sum_{i=1}^m(y(predict)_i - y_i)^2 = \\dfrac{1}{2m}\\sum_{i=1}^m(h_\\theta(x_i) - y_i)^2$\n",
    "\n",
    "    We will minimize the cost function\n",
    "\n",
    "    - The cost function (J) is a function of \\theta, hypothesis function (h) is a function of x."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Parameter Learning\n",
    "\n",
    "- **Gradient Descent**\n",
    "    - To minimize the cost function, we will apply gradient descent to our model:\n",
    "    $\\theta_j = \\theta_j - \\alpha*\\frac{\\partial}{\\partial\\theta_j}J(\\theta_0,\\theta_1)$\n",
    "\n",
    "    while $\\alpha$ is learning rate, $\\partial$ is notation of partial derivative, $\\frac{\\partial}{\\partial\\theta_j}J(\\theta_0,\\theta_1)$ is partial derivative of $J(\\theta_0,\\theta_1)$ with variable $\\theta_j$\n",
    "\n",
    "    we will repeat until \\theta is converging, this mean the partial derivative go to 0, then J(\\theta) will go to the minimum value.\n",
    "\n",
    "- **Gradient Descent for Linear Regression**\n",
    "\n",
    "    - Formula\n",
    "\n",
    "    $\\theta_0 = \\theta_0 - \\alpha\\frac{1}{m}\\sum_{i=1}^m(h_\\theta(x_i) - y_i)$\n",
    "\n",
    "    $\\theta_1 = \\theta_1 - \\alpha\\frac{1}{m}\\sum_{i=1}^m((h_\\theta(x_i) - y_i)*x_i)$\n",
    "    \n",
    "    - Explain formula: \n",
    "\n",
    "    $\\frac{1}{m}\\sum_{i=1}^m(h_\\theta(x_i) - y_i) = \\frac{\\partial}{\\partial\\theta_0}J(\\theta_0,\\theta_1)$ \n",
    "\n",
    "    $\\frac{1}{m}\\sum_{i=1}^m((h_\\theta(x_i) - y_i)x_i) = \\frac{\\partial}{\\partial\\theta_1}J(\\theta_0,\\theta_1)$ "
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}