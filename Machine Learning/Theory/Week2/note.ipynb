{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": 3
    },
    "orig_nbformat": 2,
    "colab": {
      "name": "note.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbP2RZraT-cz"
      },
      "source": [
        "## Multivariate Linear Regression\n",
        "\n",
        "- **Multiple Feature**\n",
        "    - The multivariable form of the hypothesis function:\n",
        "    $h_\\theta(x) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + ... + \\theta_nx_n$\n",
        "\n",
        "    - Using the definition of matrix multiplication:\n",
        "  \n",
        "  $h_\\theta(x) = [\\theta_0  \\theta_1 ... \\theta_n]\\begin{bmatrix}x_0\\\\x_1\\\\.. .\\\\x_n\\end{bmatrix} = \\theta^Tx$\n",
        "\n",
        "    when $x_0$ = 1\n",
        "\n",
        "- **Gradient Descent For Multiple Variables**\n",
        "\n",
        "    $\\theta_j = \\theta_j - \\alpha\\frac{1}{m}\\sum_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})x_j^{(j)}$\n",
        "\n",
        "    We will repeat until convergence.\n",
        "\n",
        "    - Remind:\n",
        "        - $x^{(i)}$, $y^{(i)}$ is traing example $i^{th}$ (row i in training set)\n",
        "        - $x_j^{(i)}$ is feature $j^{th}$ in training example $i^{th}$ (row i column j in training set)\n",
        "\n",
        "- **Gradient Descent in Practice - Feature Scaling**\n",
        "    \n",
        "    - We can speed up gradient descent by having each of our input values in roughly the same range.\n",
        "    - Two techniques to help with this are **feature scaling** and **mean normalization**\n",
        "\n",
        "    - Feature scaling involves diving the input values by the range of the input variable, resulting in a new range of just 1.\n",
        "    - Mean normalization involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for input variable for just zero:\n",
        "    \n",
        "        $x_i := \\dfrac{x_i - \\mu_i}{s_i}$\n",
        "        \n",
        "        where $\\mu_i$ is average of feature(i) and $s_i$ the range of values (max - min), or $s_i$ is the standard deviation\n",
        "\n",
        "- **Gradient Descent in Practice - Learning Rate**\n",
        "\n",
        "    - If $\\alpha$ is too small: slow convergence.\n",
        "    \n",
        "    - If $\\alpha$ is too large: may not decrease on every iteration and thus may not converge.\n",
        "\n",
        "- **Feature and Polynomial Regression**\n",
        "\n",
        "    - We can improve our feature and form of our hypothesis function in a couple different ways.\n",
        "\n",
        "    - **Polynomial Regression**\n",
        "\n",
        "        - Our hypothesis need not be linear (a straight line) if that does not fit the data well.\n",
        "\n",
        "        - We can **change the behavior or curve** of our hypothesis function by making quadratic, cubic or square root function (or any other form)\n",
        "\n",
        "## Computing Parameters Analytically\n",
        "\n",
        "- **Normal Equation**\n",
        "\n",
        "    - Let's discuss a second way of minimizing cost function (J) (the first one is gradient descent), this time performing the minimization explicity and without resorting to an iterative algorithm. In \"Normal Equation\", we will explicity taking J's derivatives with respect to the $\\theta_j$'s, and setting them to zero. This allows us to find optimum theta without iteration.\n",
        "\n",
        "    $$\\theta = (X^TX)^{-1}X^Ty$$\n",
        "\n",
        "    Where\n",
        "  $X = \\begin{bmatrix}1 && x_1^{(1)} && ... && x_n^{(1)} \\\\ 1 && x_1^{(2)} && ... && x_n^{(2)} \\\\ ... \\\\1 && x_1^{(m)} && ... && x_n^{(m)}\\end{bmatrix}$ (matrix m x (n+1)), $x_j^{(i)}$ is feature $j^{th}$ of example training $i^{th}$ and $y = \\begin{bmatrix}y^{(1)}\\\\y^{(2)}\\\\...\\\\y^{(m)}\\end{bmatrix}$ (matrix m x 1 or m-dim vector), $y^{(i)}$ is the value result (the value we will predict by hypothesis) of training example $i^{th}$.\n",
        "\n",
        "    - There is **no need** to do feature scaling with normal equation.\n",
        "\n",
        "    - When use normal equation (compare gradient descent and normal equation):\n",
        "        - **Gradient Descent**:\n",
        "\n",
        "            - Need to choose alpha.\n",
        "            - Needs many iterations.\n",
        "            - $o(kn^2)$\n",
        "            - Works well when n is large.\n",
        "\n",
        "        - **Normal equation**:\n",
        "\n",
        "            - No need to choose alpha.\n",
        "            - No need to iterate.\n",
        "            - $o(n^3)$, need to calculate inverse of $X^TX$ (matrix (n+1)x(n+1))\n",
        "            - Slow if n is very large.\n",
        "\n",
        "        - In practice, when n exceeds 10,000 it might a good time to go from a normal solution to an iterative process (gradient descent).\n",
        "\n",
        "- **Normal Equation Noninvertibility**\n",
        "\n",
        "    - When implementing the normal equation in 'octave', we want to use the 'pinv' function rather than 'inv'. The 'pinv' function will give you a value of $\\theta$ even if $X^TX$ is not inverible.\n",
        "\n",
        "    - If $X^TX$ is noninvertible, the common causes might be having:\n",
        "\n",
        "        - Redundant features, where 2 features are very closely related (i.e. they are linearly dependent).\n",
        "        \n",
        "        - Two many features (e.g. $m \\le n$). In this case, delete some features or use 'regularization'.\n",
        "\n",
        "    "
      ]
    }
  ]
}